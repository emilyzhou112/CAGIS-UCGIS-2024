---
title: "MCDA-EVSE"
author: "Emily Zhou"
date: "2024-05-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r packages and processing environment}

# list of packages required 
packages = c("tidycensus", "tidyverse", "viridis", "FNN",  "dplyr", "sf", "classInt", "readr", "ggplot2", "here", "tmap", "SpatialAcc", "svDialogs", "MCDA", "nngeo", "leaflet")

# load and install required packages
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, quietly=TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# load functions
source(here("code", "KNN.R"))

# save the R processing environment
writeLines(
  capture.output(sessionInfo()),
  here("environment", paste0("r-environment-", Sys.Date(), ".txt"))
)
```

# Study Area

```{r process study area}

philly <- st_read(here("data", "raw", "city_boundary.geojson")) %>% st_transform('ESRI:102728')
water <- st_read(here("data", "raw", "water_features.geojson")) %>% st_transform('ESRI:102728')

# generate fishnet for philadelphia
fishnet <- st_make_grid(philly,
                        cellsize = 1640, # 500 meters
                        square = TRUE) %>% 
  .[philly] %>%           
  st_sf() %>%
  mutate(uniqueID = 1:n())

# remove grids in water with 330 ft buffer
fishnet_nowater <- fishnet %>%
  filter(!(uniqueID %in% (water %>%
                            st_buffer(dist = 330) %>%
                            st_intersection(st_centroid(fishnet), .) %>%
                            st_drop_geometry() %>%
                            dplyr::select(uniqueID) %>%
                            pull(uniqueID))))

```


```{r map study grids}
ggplot() +
  geom_sf(data=fishnet, color="black", fill="blue") +
  geom_sf(data=fishnet_nowater, color="black", fill="white") +
  labs(title = "Fishnet of Philly") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, linewidth=0.8)
        )
```

# Criteria Processing 

## Demographic Information and EV Ownership

```{r load demographic info}

options(timeout=1000) 

# get API Key
# census_api_key(dlgInput("Enter a Census API Key", 
#   Sys.getenv("CENSUS_API_KEY"))$res,
#   overwrite = TRUE)

# query 2020 acs data
philly20 <- get_acs(geography = "tract", 
          variables = c(
            "B01001_001E", # total population
            "B01001_010E", # male 22-24
            "B01001_011E",
            "B01001_012E",
            "B01001_013E",
            "B01001_014E",
            "B01001_015E",
            "B01001_016E",
            "B01001_017E",
            "B01001_018E",
            "B01001_019E", # male 62-64
            "B01001_034E",
            "B01001_035E",
            "B01001_036E",
            "B01001_037E",
            "B01001_038E",
            "B01001_039E",
            "B01001_040E",
            "B01001_041E",
            "B01001_042E",
            "B01001_043E" #female 62-64
            ), 
          year=2020, state="PA", county="Philadelphia", 
          geometry=TRUE, output="wide") %>%
          st_transform('ESRI:102728')

philly20 <- philly20 %>%
  mutate(popden = B01001_001E / (st_area(geometry) * 9.2903e-8),
         density = as.integer(gsub("\\[.*\\]", "", popden)),
         pop22_64 = B01001_010E + B01001_011E + B01001_012E + B01001_013E + B01001_014E +
                    B01001_015E + B01001_016E + B01001_017E + B01001_018E + B01001_019E +
                    B01001_034E + B01001_035E + B01001_036E + B01001_037E + B01001_038E + 
                    B01001_039E + B01001_040E + B01001_041E + B01001_042E + B01001_043E,
         driving_pop = pop22_64 / B01001_001E,
         totalpop = B01001_001E) %>%
  dplyr::select(GEOID, totalpop, density, driving_pop) %>%
  mutate(driving_pop = ifelse(is.nan(driving_pop), 0, driving_pop))


```


```{r load registered ev}

reg_ev <- st_read(here("data", "raw", "reg_ev.geojson"))
Zip_Code <- as.character(c(
  19120, 19124, 19111, 19143, 19149, 19134, 19140, 19148, 19104, 19144,
  19145, 19131, 19139, 19146, 19147, 19115, 19136, 19128, 19135, 19121,
  19154, 19141, 19132, 19152, 19114, 19116, 19151, 19138, 19142, 19119,
  19130, 19125, 19133, 19103, 19150, 19122, 19126, 19123, 19107, 19106,
  19153, 19129, 19118, 19137, 19127, 19102, 19108, 19109, 19176, 19112,
  19160, 19155, 19162, 19161, 19171, 19170, 19173, 19172, 19175, 19178,
  19177, 19181, 19179, 19183, 19182, 19185, 19184, 19188, 19187, 19192,
  19191, 19194, 19193, 19196, 19195, 19244, 19197, 19255, 19019, 19093,
  19092, 19101, 19099, 19105, 19110, 19190
))
reg_ev <- reg_ev %>% 
  filter(zip %in% Zip_Code) %>% 
  dplyr::select(elec_cnt) %>% 
  st_transform('ESRI:102728')

```



## Existing EVSE Distribution and Access

```{r load existing evse}

evse <- read.csv(here("data", "raw", "existing_evse.csv"))
evse <- evse %>% 
  filter(City == "Philadelphia") %>% 
  dplyr::select(Latitude, Longitude, EV.Level2.EVSE.Num) %>% 
  filter(is.na(Latitude) == FALSE & is.na(Longitude) == FALSE) %>% 
  st_as_sf(., coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  st_transform('ESRI:102728') %>% 
  mutate(num = ifelse(is.na(EV.Level2.EVSE.Num), 1, EV.Level2.EVSE.Num))

```


```{r compute spatial accessibility}

tracts.coords <- st_coordinates(st_centroid(philly20))
evse.coords <- st_coordinates(evse)
dist.matrix <- distance(tracts.coords, evse.coords, type = "euclidean")

TSFCA <- ac(p = philly20$totalpop, 
            n = evse$num, 
            D = dist.matrix, d0 = 2000, family = "2SFCA")
philly20 <- philly20 %>% 
  mutate(TSFCA = TSFCA)
```


## Other Considerations

```{r}

parking <- st_read(here("data", "raw", "osm_features", "parking.geojson"))
parking <- parking %>% 
  filter(!access %in% c("no", "permissive", "permit", "private")) %>% 
  st_transform('ESRI:102728')

police_fire <- st_read(here("data", "raw", "osm_features", "police_fire.geojson")) 
police_fire <- police_fire %>% 
  st_transform('ESRI:102728') %>% 
  st_centroid()

zoning <- st_read(here("data", "raw", "zoning", "zoning.shp"))
zoning <- zoning %>% 
  st_transform('ESRI:102728') %>% 
  mutate(zoning = case_when( ZONINGGROU == "Commercial/Commercial Mixed-Use" ~ "Commercial",
                             ZONINGGROU == "Industrial/Industrial Mixed-Use" ~ "Industrial",
                             ZONINGGROU == "Residential/Residential Mixed-Use" ~ "Residential",
                             ZONINGGROU == "Special Purpose" ~ "Special",
                             TRUE ~ "Undefined")) %>% 
  mutate(zone_score = case_when( zoning == "Commercial" ~ 0.4,
                             zoning == "Industrial" ~ 0.15,
                             zoning == "Residential" ~ 0.3,
                             TRUE ~ 0.15))

electricity <- read.csv(here("data", "raw", "property-electricity-data.csv")) %>% 
  dplyr::select(Y = y_coord, X = x_coord, portfolio_bldg_id, postal_code, primary_property_type, sector, electricity_2021) %>%
  na.omit() %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102728') %>% 
  st_intersection(., fishnet) # clip to fishnet extent

```


# Aggregate Criteria

```{r aggregate all criteria into fishnet}

net_centroid <- st_centroid(fishnet_nowater)
fishnet_nowater <- fishnet_nowater %>%
  left_join(net_centroid %>% 
              mutate(evse.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(evse), 2)*0.3048) %>% # distance to evse
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              st_intersection(philly20) %>% 
              st_drop_geometry(), by = "uniqueID") %>% # demographic info and spatial acc
  left_join(net_centroid %>% 
              st_intersection(reg_ev) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              mutate(parking.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(parking), 2)*0.3048) %>% # dist to parking
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              mutate(policefire.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(police_fire), 1)*0.3048) %>% # police and fire
              st_drop_geometry(),by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              st_buffer(dist = 200) %>% 
              st_intersection(zoning) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  mutate(TSFCA = ifelse(is.infinite(TSFCA), 0, TSFCA)) %>% 
  mutate(elec_cnt = ifelse(is.na(elec_cnt), 0, elec_cnt)) %>% 
  filter(is.na(GEOID) == FALSE) %>% 
  filter(is.na(zone_score) == FALSE) %>% 
  dplyr::select(-c(GEOID, totalpop, featCount, ZONINGGROU, zoning)) %>% 
  group_by(uniqueID) %>% # for duplicate grids, take the mean of the zone score
  mutate(zone_score = mean(zone_score)) %>%
  ungroup() %>%
  distinct(uniqueID, .keep_all = TRUE)
  
    
```


```{r aggregate power grid}

# buffer the building and aggregate building centroid into the buffer
# if outside the buffer, get the nearest neighbor
elec_grid <- st_join(net_centroid, 
                electricity %>% st_buffer(dist = 1640)) %>% 
  group_by(uniqueID.x) %>% 
  summarize(elect_use = mean(electricity_2021))

unjoin <- elec_grid %>% 
  filter(is.na(elect_use)) %>% 
  mutate(neighbor_index = 1:n()) 
  
nn_indices <- st_nn(unjoin,
                    electricity, k = 1)
nearest_neighbors <- electricity[unlist(nn_indices), ] %>% mutate(neighbor_index = 1:n())

unjoin <- unjoin %>%
  st_drop_geometry() %>% 
  left_join(nearest_neighbors %>% select(electricity_2021, neighbor_index), 
            by = c("neighbor_index" = "neighbor_index"))

elec_grid <- elec_grid %>% 
  left_join(., unjoin %>% 
              dplyr::select(uniqueID.x, electricity_2021), by = c("uniqueID.x" = "uniqueID.x")) %>% 
  mutate(elect_use = case_when(
    is.na(elect_use) ~ electricity_2021,
    TRUE ~ elect_use
  )) %>% 
  dplyr::select(-electricity_2021) %>% 
  st_drop_geometry()

fishnet_nowater <- fishnet_nowater %>% 
  left_join(., elec_grid, by = c("uniqueID" = "uniqueID.x") ) %>% 
  # filter(uniqueID != 1592,)
```


# Multi-Criteria Decision Analysis

- Transform indicator data into standard **geographic unit**. This include: aggregate or dissolve for nested relationship, area-weighted re-aggregation for un-nested relationship, count points within polygons, etc. 
- Transform indicator data into standard **measuring unit**. This include: rank or percentile, z-score, some other functions. 
- Combine indicator data into a composite score with certain weights. 

## Self-Assigned Weight + WSM

```{r}

scale_values <- function(x){(x-min(x))/(max(x)-min(x))}
WSM <- fishnet_nowater %>% 
  mutate(scl_density = scale_values(density),
         scl_acc = scale_values(TSFCA),
         scl_drivingpop = scale_values(driving_pop),
         scl_evse = scale_values(evse.nn),
         scl_elec = scale_values(elec_cnt),
         scl_parking = scale_values(parking.nn),
         scl_policefire = scale_values(policefire.nn),
         scl_elect_use = scale_values(elect_use)
         ) %>% # now inverse some scales
  mutate(scl_evse_re = 0 - scl_evse + 1,
         scl_acc_re = 0 - scl_acc + 1, 
         scl_parking_re = 0 - scl_parking + 1,
         scl_policefire_re = 0 - scl_policefire + 1) %>% 
  mutate(score = 0.12*scl_density + 0.18*scl_evse_re + 0.15*scl_elec + 0.1*scl_drivingpop + 0.1*scl_parking_re + 0.05*scl_policefire_re + 0.05*zone_score + 0.12*scl_acc_re +0.13*scl_elect_use) # self assign weight
```

South Philadelphia stands out. 

```{r}

#quantiles <- classIntervals(WSM$score, n = 8, style = "quantile")
#WSM$quantile <- cut(WSM$score, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=WSM, color="black", aes(fill=score)) #+ scale_fill_brewer(palette = "Blues")
```

## Self Assigned Weight + Classical TOPSIS

This approach is employed for four main reasons:
- Logic is rational and understandable
- The computation process is straightforward
- The concept permits the pursuit of the best alternatives for each criterion depicted in a simple mathematicl form.
- The importance weights are incorporated into the comparison procedure. 

1. Construct the decision matrix where each element represents the performance score of each alternatives.
2. Normalization
3. Determine the ideal and negative ideal solution. In other words, the best and worst possible value for each criterion. 
4. Calculate the separation measure as in euclidean distance calculation. 
5. Measure relative closeness: how close each alternative is to the ideal solution relative to its distance from the negative-ideal solution. 
6. Ranking 

```{r}

performanceTable <- fishnet_nowater %>% 
  dplyr::select(-c(uniqueID)) %>% 
  st_drop_geometry() %>% 
  mutate(density = scale_values(density),
         TSFCA = scale_values(TSFCA),
         driving_pop = scale_values(driving_pop),
         evse.nn = scale_values(evse.nn),
         elec_cnt = scale_values(elec_cnt),
         parking.nn = scale_values(parking.nn),
         policefire.nn = scale_values(policefire.nn),
         elect_use = scale_values(elect_use)
         )
uniqueIDs <- fishnet_nowater$uniqueID
rownames(performanceTable) <- uniqueIDs
```

```{r}
# come up with a positive ideal solutions
summary(performanceTable$evse.nn)
summary(performanceTable$density)
summary(performanceTable$driving_pop) 
summary(performanceTable$TSFCA) 
summary(performanceTable$elec_cnt) 
summary(performanceTable$parking.nn) 
summary(performanceTable$policefire.nn) 
summary(performanceTable$zone_score) 
summary(performanceTable$elect_use) 

```

```{r}

# reverse is min
weights <- c(0.18, 0.12, 0.1, 0.12, 0.15, 0.1, 0.05, 0.05, 0.13)
criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
positiveIdealSolutions <- c(0.17, 0.60, 0.70, 0, 0.50, 0.10, 0.14, 0.3,0.007)
overall1 <- TOPSIS(performanceTable, weights, criteriaMinMax, positiveIdealSolutions)
TOPSIS <- data.frame(uniqueID = names(overall1), values = overall1) %>%  
  mutate(uniqueID = as.integer(uniqueID))
  
TOPSIS_toplot <- fishnet_nowater %>% 
  left_join(TOPSIS, by = "uniqueID")

```


```{r}

quantiles <- classIntervals(TOPSIS_toplot$values, n = 8, style = "quantile")
TOPSIS_toplot$quantile <- cut(TOPSIS_toplot$values, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=TOPSIS_toplot, color="black", aes(fill=quantile))+ scale_fill_brewer(palette = "RdBu")
```

However, the TOPSIS method presents certain drawbacks. One of the problems attributable to TOPSIS is that it can cause the phenomenon known as rank reversal. In this phenomenon the alternativesâ€™ order of preference changes when an alternative is added to or removed from the decision problem. In some cases this may lead to what is called total rank reversal, where the order of preferences is totally inverted, that is to say, that the alternative considered the best, with the inclusion or removal of an alternative from the process, then becomes the worst. 

```{r}
summary(TOPSIS$values)
```

## Self Assigned Weights + PROMETHEE

1. Calculate the difference in performance scores for each criterion between all pairs of alternatives.
2. Set up an indifference threshold.
3. Set up a preference threshold. 
4. Decide a preference function
5. Apply the preference function using the indifference and preference threshold to convert the differences into preference value, which is typically betweeen 0 and 1. 
6. For each alternative, sum the preference values across all criteria to obtain the aggregated preference indices.
7. Calculate the positive and negative outranking flows for each alternative, which represent how much an alternative outranks others and is outranked by others, respectively.
8. Use the net outranking flow (positive flow minus negative flow) to rank the alternatives.

The gaussParameter vector in the code specifies the standard deviation (s) for the Gaussian preference function for each criterion.

Terminology: 
- preference function: define how much better one site is better over another for each criterion (a linear function where the lower the accessibility, the better)

- preference threshold: the minimum difference between the performance of two alternatives on a given criterion such that the decision-maker has a strict preference for one alternative over the other. **If the performance difference is greater than or equal to p, the decision-maker clearly prefers one alternative over the other for that criterion. **

- indifference threshold: The indifference threshold q represents the maximum difference between the performance of two alternatives on a given criterion such that the decision-maker is indifferent between them. **If the performance difference is less than or equal to q, the decision-maker considers the two alternatives to be effectively equal with respect to that criterion.**

Now, this positive and negative ideal scenario is based solely on quantitative analysis on the distribution of the data. 
```{r}

IQR(performanceTable$evse.nn) / 3
IQR(performanceTable$density) / 3
IQR(performanceTable$driving_pop) /3 
IQR(performanceTable$TSFCA) / 3 
IQR(performanceTable$elec_cnt) / 3
IQR(performanceTable$parking.nn) / 3
IQR(performanceTable$policefire.nn) / 3
IQR(performanceTable$zone_score) / 3
IQR(performanceTable$elect_use) / 3

```

```{r}

criteriaWeights <- c(0.18, 0.12, 0.1, 0.12, 0.15, 0.1, 0.05, 0.05, 0.13)
names(criteriaWeights)<-colnames(performanceTable)

criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
names(criteriaMinMax)<-colnames(performanceTable)

preferenceFunction<-c("Usual","U-shape","V-shape","Level","V-shape-Indiff","Gaussian", "Level","V-shape-Indiff", "Level")
gaussParameter<-c(0.25,1,2,0,0,0,0,0,0)
names(gaussParameter)<-colnames(performanceTable)

#Preference threshold
preferenceThreshold<-c(0.31, 0.21, 0.12, 0, 0.17, 0.19, 0.23, 0.075, 0.005367344)
names(preferenceThreshold)<-colnames(performanceTable)

#Indifference threshold
indifferenceThreshold<-c(0.1, 0.07, 0.041, 0, 0.058, 0.065, 0.078, 0.025, 0.001789115)
names(indifferenceThreshold)<-colnames(performanceTable)


```


```{r}
performanceTable <- as.matrix(performanceTable)
```


```{r}

promethee <- PROMETHEEOutrankingFlows(performanceTable, preferenceFunction,preferenceThreshold,
indifferenceThreshold,gaussParameter,criteriaWeights,criteriaMinMax)
```


```{r}


promethee_df <- data.frame(uniqueID = names(promethee$outrankingFlowsPos), 
           Pos = unname(promethee$outrankingFlowsPos),
           Neg = unname(promethee$outrankingFlowsNeg)) %>%  
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  mutate(net = Pos-Neg) %>% 
  dplyr::select(uniqueID, net)

PROMETHEE_toplot <- fishnet_nowater %>% 
  left_join(promethee_df, by = "uniqueID")
  
```


```{r}
#quantiles <- classIntervals(PROMETHEE_toplot$net, n = 8, style = "quantile")
#PROMETHEE_toplot$quantile <- cut(PROMETHEE_toplot$net, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=PROMETHEE_toplot, color="black", aes(fill=net)) #+ scale_fill_brewer(palette = "Blues")
```

## AHP + TOPSIS


```{r}

crit <- c("density","evse","ev", "drivingpop", "parking", "policefire", "zoning", "acc", "electricity")

criteriaWeightsPairwiseComparisons <- matrix(c(1.0, 3.0, 2.0, 1/2, 1/2, 1/4, 1/3, 1.5, 1.2,
                                               1/3, 1.0, 1/1.2, 1/4, 1/4, 1/8, 1/6, 1/2, 1/1.5, 
                                               1/2, 1.2, 1.0, 1/3, 1/3, 1/6, 1/6, 1/2, 1/1.5, 
                                               2, 4, 3, 1.0, 1/1.2, 1/4, 1/5, 1.5, 1.8,
                                               2, 4, 3, 1.2, 1.0, 1/4, 1/4, 2, 1.5, 
                                               4, 8, 6, 4, 4, 1, 1.2, 3, 2,
                                               3, 6, 6, 5, 4, 1/1.2, 1, 3, 2,
                                               1/1,5, 2, 2, 1/1.5, 1/2, 1/3, 1/3, 1, 1.5, 
                                               1/1.2, 1.5, 1.5, 1/1.8, 1/1.5, 1/2, 1/2, 1/1.5, 1
                                               ),
nrow=length(crit),
ncol=length(crit),
dimnames=list(crit,crit))


```

```{r}

column_sums <- colSums(criteriaWeightsPairwiseComparisons)

# Divide each element by the sum of its column
normalized_matrix <- sweep(criteriaWeightsPairwiseComparisons, 2, column_sums, FUN = "/")

# Step 3: Calculate the weights
# Calculate the average of each row
weights <- rowMeans(normalized_matrix)

# Output the weights
print(weights)

```

```{r}

# reverse is min
weights <- c(0.249, 0.114, 0.105, 0.098, 0.1909, 0.074, 0.033, 0.033, 0.102)
criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
# positiveIdealSolutions <- c(0.17, 0.60, 0.70, 0, 0.50, 0.10, 0.14, 0.3,0.007)
overall2 <- TOPSIS(performanceTable, weights, criteriaMinMax)
AHP_TOPSIS <- data.frame(uniqueID = names(overall2), values = overall2) %>%  
  mutate(uniqueID = as.integer(uniqueID))
  
AHP_TOPSIS_toplot <- fishnet_nowater %>% 
  left_join(TOPSIS, by = "uniqueID") 


```


```{r}

quantiles <- classIntervals(AHP_TOPSIS_toplot$values, n = 8, style = "quantile")
AHP_TOPSIS_toplot$quantile <- cut(AHP_TOPSIS_toplot$values, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=AHP_TOPSIS_toplot, color="black", aes(fill=quantile)) + scale_fill_brewer(palette = "RdBu")
```

# Consistency Check 


```{r}

TOPSIS <- TOPSIS %>%
  mutate(topsis_rank = min_rank(desc(values)))

WSM <- WSM %>% 
  dplyr::select(c(uniqueID, score)) %>% 
  mutate(wsm_rank = min_rank(desc(score)))

PROMETHEE <- promethee_df %>% 
  mutate(promethee_rank = min_rank(desc(net)))

AHP_TOPSIS <- AHP_TOPSIS %>% 
  mutate(ahptopsis_rank = min_rank(desc(values)))

rank_compare <- WSM %>% 
  st_drop_geometry() %>% 
  dplyr::select(c(uniqueID, wsm_rank)) %>% 
  left_join(., TOPSIS %>% dplyr::select(uniqueID, topsis_rank), by = "uniqueID") %>% 
  left_join(., PROMETHEE %>% dplyr::select(uniqueID, promethee_rank), by = "uniqueID") %>% 
  left_join(., AHP_TOPSIS %>% dplyr::select(uniqueID, ahptopsis_rank), by = "uniqueID") %>% 
  mutate(avg_rank = as.integer((wsm_rank + topsis_rank + promethee_rank + ahptopsis_rank) / 4))

```

We took the average rank among all methods
We would like to find places in Philadelphia that are consistently ranked more suitable than other. 

```{r}
rank_toplot <- fishnet_nowater %>% 
  left_join(rank_compare, by = "uniqueID")
ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=rank_toplot, color="black", aes(fill=avg_rank))
```

## Further Analysis

We would like to see if some neighborhoods are more suitable than others overall. 

```{r}
neighborhood <- st_read(here("data", "raw", "PhillyPlanning_Neighborhoods", "PhillyPlanning_Neighborhoods.shp")) %>% st_transform('ESRI:102728') 
  

final <- fishnet_nowater %>% 
  left_join(., rank_compare, by = "uniqueID") %>% 
  left_join(net_centroid %>% 
               st_intersection(neighborhood %>% dplyr::select(NAME)) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  mutate(NAME = ifelse(is.na(NAME), "NOT APPLICABLE", NAME))
  
```


```{r}

final %>% 
  st_drop_geometry() %>% 
  group_by(NAME) %>% 
  summarize(overall_rank = mean(avg_rank)) %>% 
  left_join(neighborhood, .,by = "NAME") %>% 
  ggplot()+
  geom_sf(color="black", aes(fill=overall_rank))
  
```
Distribution of differences in rank between different methods:

Comparing across topsis, promethee, and weighted sum in ranking the alternatives:
- All methods rank alternatives differently. 
- The differences in rank between weighted sum and promethee method is the smallest. 
- Comparing topsis to weighted sum method and the promethee method, rank reversal is definitely an issue. There are a few grids where TOPSIS significantly over-rank and/or under-rank grids. 
- comparing self-assigned weight method with using the ahp, there are a lot of positive values, meaning that self-assigned weight has under-ranked sites than using the ahp. 


```{r}

# if negative, it means that wsm over-rank topsis or promethee
# if positive, it means that wsm under-rank topsis or promethee
compare<- final %>% 
  mutate(wsm_topsis = wsm_rank - topsis_rank,
         wsm_promethee = wsm_rank - promethee_rank,
         topsis_promethee = topsis_rank - promethee_rank) %>% 
  mutate(compare_weight = topsis_rank - ahptopsis_rank)

compare %>% 
  st_drop_geometry() %>% 
  dplyr::select(c(uniqueID, wsm_topsis, wsm_promethee, topsis_promethee, compare_weight, NAME)) %>% 
  pivot_longer(cols = c("wsm_topsis", "wsm_promethee", "topsis_promethee", "compare_weight"),  
               names_to = "variable",        # Name for the new 'variable' column
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Differences in Rank Between Different Methods",
       x = "Difference",
       y = "Frequency") +
  theme_minimal() +
  facet_wrap(~ variable)
  

```

Closer examinations reveal that for places where promethee and wsm both give a low rank, the outskirt of philadelpia, topsis give a higher rank and vice versa. TOPSIS can also be sensitive to the addition of new criteria. 

When a new alternative is introduced or an existing one is removed, the distances to the ideal and negative-ideal solutions can change. The presence of extreme values (very high or very low) can significantly influence the ideal and negative-ideal solutions. Assues independence of criteria. 

```{r fig.height=14, fig.width=12}

compare %>% 
  dplyr::select(c(uniqueID, wsm_topsis, wsm_promethee, topsis_promethee, compare_weight, NAME)) %>% 
  pivot_longer(cols = c("wsm_topsis", "wsm_promethee", "topsis_promethee", "compare_weight"),  
               names_to = "variable",        # Name for the new 'variable' column
               values_to = "value") %>% 
  ggplot() +
  geom_sf(aes(fill = value), color = "black") +
  scale_fill_distiller(palette = "RdBu", direction = 1, 
                       name = "Sample Variable", 
                       na.value = "grey50") +
  labs(title = "Map with Divergent Color Ramp") +
  theme_minimal() +
  facet_wrap(~ variable)

```


Comparing the MCDA Method
WSM:
- Easy to understand and implement. 
- Only requires determining the weights. 
- Assumes that criteria are linearly additive.
- Results heavily depend on the assigned weights. 

PROMETHEE:
- Allows for detailed preference modeling with different preference function. 
- Introduce some forms of pair-wise comparison between alternatives. 
- Requires careful selection of preference functions and preference function, indifference threshold 
- Computationally intensive

TOPSIS:
- Easy to understand and implement.
- Assumes that the criteria are independent of each other
- Suffer from reversal rank problem. 
- Require deciding upon the positive and negative ideal scenario. 

Comparing weighting Method
AHP:
- a structured approach that breaks complex decisions into manageable parts. 
- pairwise comparison can be subjective and biased 
- maintain consistency in comparison can be challenging. 


Takeaways:

1. Our study provided a spatially informed starting point to identify potential areas for new EVSE that contribute to Philadelphia's sustainable transportation goals. 

2. Our study highlighted the challenges in agreeing on the input criteria and weighting scheme and that different weighting scheme could significantly alter the result. Considering multiple goals here, including equity,sustainable transportation, affordability, etc.,  what should be included and how important they are become important questions to consider.  

3. There's no perfect decision making model. Methods that are more complicated and robust mathematically 1) requires more decision inputs, which is more time consuming on the stakeholder side and introduces more subjectivity 2) could be more computationally intensive, 3) less intuitive to non-experts. 

4. The state of GIS model for decision making. We need to decide upon the best possible course of action for the future, often with incomplete information and competing/conflicting interests at stake. The best practice is to not rely on a single method in decision making. Beyond using MCDA models, it is essential to check the actual site condition before making final decisions. Note that there are things that spatial analysis cannot capture. 

5. Our study have also highlighted several recurrent challenges in geospatial model for decision making. Specifically, the modifiable areal unit problem (MAUP) is a source of uncertainty, considering that out raw data all comes with different spatial unit and needs to be re-aggregated before proceed. In addition, ecological fallacy could be an issues as we are making inference about a small portion of the population using tract level statistics for the whole population.Moreover, errors and uncertainties may also arise when dealing with missing information, when we make assumptions/spatial interpolations about a neighboring grids' situation. 

6. The use of a MCDM framework based on characterization of alternatives could inform short term choices considering known constraints and preferences. Moving forward, we would like to look for ways to bridge the gap between short term decision making and long-term planning in a context of rapidly changing projections regarding future states of EV usage/climate scenarios. We believe the innovation lies in developing a structured and replicable framework that can provide a model for other cities facing similar challenges. 


# Public Private Partnership

```{r}

acme <- read.csv(here("data", "raw", "acme.csv")) %>% 
  st_as_sf(., coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  mutate(longitude= unlist(map(geometry, 1)),
         latitude = unlist(map(geometry, 2)))

```


```{r}


pal <- colorBin(palette = "Blues", # change the color palette here
                domain = final$avg_rank, # change the parameter name after $
                bins = c(0, 200, 400, 600, 800, 1000, 1200)) # change the bins here 


labs <- paste0("<strong>", final$NAME,
               "</strong><br/>Avg Rank: ", final$avg_rank, 
               "</strong><br/>WSM Rank: ", final$wsm_rank, 
               "</strong><br/>TOPSIS Rank: ", final$topsis_rank, 
               "</strong><br/>PROMETHEE Rank: ", final$promethee_rank, 
               "</strong><br/>AHP-TOPSIS Rank: ", final$ahptopsis_rank
               ) 

labs_acme <- paste0("</strong><br/>ACME Address ", acme$Address)

```

```{r}
leaflet <- final %>%
  st_transform("EPSG:4326") %>% 
  leaflet() %>%
  setView(lng = -75.1652, lat = 40.0126, zoom = 10.5) %>%
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addPolygons(weight = 1,
              fillColor = ~pal(avg_rank), # change the name of the parameter in the bracket
              popup = labs,
              fillOpacity = 1) %>%
  addCircleMarkers(
    data = acme,  # Replace with your actual point data
    lng = ~longitude,  # Replace with the appropriate variable in your point dataset for longitude
    lat = ~latitude,  # Replace with the appropriate variable in your point dataset for latitude
    weight = 0.5,
    radius = 4,
    fillColor = "red",  # Set the fill color for the markers
    fillOpacity = 1,
    popup = ~labs_acme  # Replace with the appropriate variable in your point dataset for popup text
  ) %>% 
  addLegend(position = "bottomright",
            pal = pal,
            values = ~avg_rank, # change the name of the parameter after ~
            title = "EVSE Suitability Rank") # change the name of the legend 

leaflet
```











