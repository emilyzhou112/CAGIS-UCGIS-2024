---
title: "Advancing Sustainable Mobility: A GIS-Based Multi-Criteria Decision Analysis for Equitable Electric Vehicle Supply Equipment Deployment in Philadelphia"
author: "Emily Zhou, Junyi Yang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: simplex
    toc: yes
    toc_float: yes
    code_folding: hide
    code_download: yes
editor_options:
  markdown:
    wrap: sentence
---

Version 2.0 \| First Created Nov 17, 2023 \| Updated Jun 2, 2024

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract 

The transition from traditional fossil fuel vehicles to electric vehicles (EVs) stands out as a pivotal solution to decarbonize the transportation systems and combat the climate crisis. However, the widespread adoption of EVs faces challenges, with one significant obstacle being the expansion of electric vehicle supply equipment (EVSE). The City of Philadelphia’s Office of Innovation Technology (OIT) is actively engaged in developing a network of extensive, equitable, and accessible EVSEs as part of its Smart City initiatives. Despite these efforts, the initiative faces a common challenge related to the selection of suitable sites.

We share a geographic information system (GIS)-based multi-criteria decision analysis (MCDA) method that can be used to evaluate the suitability of potential EVSE sites to support the sustainable and equitable deployment of EVSE in Philadelphia. Our MCDA approach considers key criteria ranging from socio-demographic indicators (e.g. driving-age population) to site-specific characteristics (e.g. spatial accessibility of existing EVSEs, availability of public parking garages, and city-wide power grid) and is based on the Analytic Hierarchy Process (AHP). To identify the optimal sites, three MCDA methods – WSM (weighted sum method), PROMETHEE (Method for Organizing Ranking of Preferences for Enrichment), as well as TOPSIS (Technique for Order Preference by Similarity to Ideal Solutions) – are applied and compared. 

We implemented our study with open-source R markdown and acquired data from the American Community Survey (ACS), OpenStreetMap, and the US Department of Energy. Specifically, a five-step solution approach is developed for the problem: 1) creating a fishnet for Philadelphia as the spatial unit for analysis and removing any water features, 2) determining and preprocessing criteria, among which we calculated the spatial accessibility of EVSEs using 2SFCA and distance to parking lots using k-nearest neighbor, 3) aggregating criteria into the fishnet, 4) prioritizing the criteria using AHP and finally 5) ranking the potential sites using WSM, PROMETHEE, and TOPSIS. 

The results of our MCDAs highlighted several areas in Philadelphia with a pronounced demand for new EVSE. We selected one of those sites in South Philadelphia and proposed a public-private partnership model between Philadelphia’s OIT and local grocery stores to install and maintain the EVSEs. We subsequently conducted financial analyses for the cost and revenue of breakdowns and designed a phased implementation of EVSE infrastructure given the current site conditions. Our study highlighted the challenges of agreeing on the input criteria and weighting schemes in MCDA but provided a spatially informed starting point to identify sites for new EVSE that contribute to Philadelphia’s sustainable transportation goals while addressing social disparities. It also provides a scalable and replicable model for other cities facing similar challenges in deploying EVSE and advancing smart city initiatives. More importantly, it emphasized the potential of geospatial analysis in shaping a more climate-smart and equitable future. 

The study is conducted collaboratively by graduate students from the Weitzman School of Design and Wharton Business School at the University of Pennsylvania and is available as a reproducible repository on [GitHub](https://github.com/emilyzhou112/CAGIS-UCGIS-2024).

## Keywords

Multi-Criteria Decision Analysis, Analytic Hierarchy Process, Electric Vehicle Supply Equipment, Sustainable Transportation, Smart City, GIS

# Setup

```{r packages and processing environment}

# list of packages required 
packages = c("tidycensus", "tidyverse", "viridis", "FNN",  "dplyr", "sf", "classInt", "readr", "ggplot2", "here", "tmap", "SpatialAcc", "svDialogs", "MCDA", "nngeo", "leaflet", "gghalves")

# load and install required packages
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE, quietly=TRUE)
      library(x, character.only = TRUE)
    }
  }
)

# load functions
source(here("code", "KNN.R"))

# save the R processing environment
writeLines(
  capture.output(sessionInfo()),
  here("environment", paste0("r-environment-", Sys.Date(), ".txt"))
)
```

# Study Area

```{r process study area, warning=FALSE, message=FALSE}

philly <- st_read(here("data", "raw", "city_boundary.geojson")) %>% st_transform('ESRI:102728')
water <- st_read(here("data", "raw", "water_features.geojson")) %>% st_transform('ESRI:102728')

# generate fishnet for Philadelphia
fishnet <- st_make_grid(philly,
                        cellsize = 1640, # 500 meters
                        square = TRUE) %>% 
  .[philly] %>%           
  st_sf() %>%
  mutate(uniqueID = 1:n())

# remove grids in water with 330 ft buffer
fishnet_nowater <- fishnet %>%
  filter(!(uniqueID %in% (water %>%
                            st_buffer(dist = 330) %>%
                            st_intersection(st_centroid(fishnet), .) %>%
                            st_drop_geometry() %>%
                            dplyr::select(uniqueID) %>%
                            pull(uniqueID))))

```


```{r map study grids}
ggplot() +
  geom_sf(data=fishnet, color="black", fill="#726DA8") +
  geom_sf(data=fishnet_nowater, color="black", fill="white") +
  labs(title = "Fishnet of Philly") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, linewidth=0.8)
        )
```

```{r save the intermediaries}

st_write(fishnet, here("data", "derived", "philly_fishnet.geojson"), driver = "GeoJSON")
st_write(fishnet_nowater, here("data", "derived", "fishnet_nowater.geojson"), driver = "GeoJSON")

```

# Criteria Processing 

## Demographic Information and EV Ownership

```{r load demographic info, message=FALSE, warning=FALSE}

options(timeout=1000) 

# get API Key
census_api_key(dlgInput("Enter a Census API Key", 
   Sys.getenv("CENSUS_API_KEY"))$res,
   overwrite = TRUE)

# query 2020 acs data
philly20 <- get_acs(geography = "tract", 
          variables = c(
            "B01001_001E", # total population
            "B01001_010E", # male 22-24
            "B01001_011E",
            "B01001_012E",
            "B01001_013E",
            "B01001_014E",
            "B01001_015E",
            "B01001_016E",
            "B01001_017E",
            "B01001_018E",
            "B01001_019E", # male 62-64
            "B01001_034E",
            "B01001_035E",
            "B01001_036E",
            "B01001_037E",
            "B01001_038E",
            "B01001_039E",
            "B01001_040E",
            "B01001_041E",
            "B01001_042E",
            "B01001_043E" #female 62-64
            ), 
          year=2020, state="PA", county="Philadelphia", 
          geometry=TRUE, output="wide") %>%
          st_transform('ESRI:102728')

philly20 <- philly20 %>%
  mutate(popden = B01001_001E / (st_area(geometry) * 9.2903e-8),
         density = as.integer(gsub("\\[.*\\]", "", popden)),
         pop22_64 = B01001_010E + B01001_011E + B01001_012E + B01001_013E + B01001_014E +
                    B01001_015E + B01001_016E + B01001_017E + B01001_018E + B01001_019E +
                    B01001_034E + B01001_035E + B01001_036E + B01001_037E + B01001_038E + 
                    B01001_039E + B01001_040E + B01001_041E + B01001_042E + B01001_043E,
         driving_pop = pop22_64 / B01001_001E,
         totalpop = B01001_001E) %>%
  dplyr::select(GEOID, totalpop, density, driving_pop) %>%
  mutate(driving_pop = ifelse(is.nan(driving_pop), 0, driving_pop))


```


```{r save the queried acs information}

st_write(philly20, here("data", "derived", "acs2020.geojson"), driver = "GeoJSON")

```


```{r load registered ev, message=FALSE, warning=FALSE}

reg_ev <- st_read(here("data", "raw", "reg_ev.geojson"))
Zip_Code <- as.character(c(
  19120, 19124, 19111, 19143, 19149, 19134, 19140, 19148, 19104, 19144,
  19145, 19131, 19139, 19146, 19147, 19115, 19136, 19128, 19135, 19121,
  19154, 19141, 19132, 19152, 19114, 19116, 19151, 19138, 19142, 19119,
  19130, 19125, 19133, 19103, 19150, 19122, 19126, 19123, 19107, 19106,
  19153, 19129, 19118, 19137, 19127, 19102, 19108, 19109, 19176, 19112,
  19160, 19155, 19162, 19161, 19171, 19170, 19173, 19172, 19175, 19178,
  19177, 19181, 19179, 19183, 19182, 19185, 19184, 19188, 19187, 19192,
  19191, 19194, 19193, 19196, 19195, 19244, 19197, 19255, 19019, 19093,
  19092, 19101, 19099, 19105, 19110, 19190
))
reg_ev <- reg_ev %>% 
  filter(zip %in% Zip_Code) %>% 
  dplyr::select(elec_cnt) %>% 
  st_transform('ESRI:102728')

```


## Existing EVSE Distribution and Access

```{r load existing evse, message=FALSE, warning=FALSE}

evse <- read.csv(here("data", "raw", "existing_evse.csv"))
evse <- evse %>% 
  filter(City == "Philadelphia") %>% 
  dplyr::select(Latitude, Longitude, EV.Level2.EVSE.Num) %>% 
  filter(is.na(Latitude) == FALSE & is.na(Longitude) == FALSE) %>% 
  st_as_sf(., coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  st_transform('ESRI:102728') %>% 
  mutate(num = ifelse(is.na(EV.Level2.EVSE.Num), 1, EV.Level2.EVSE.Num))

```


```{r compute spatial accessibility, message=FALSE, warning=FALSE}

tracts.coords <- st_coordinates(st_centroid(philly20))
evse.coords <- st_coordinates(evse)
dist.matrix <- distance(tracts.coords, evse.coords, type = "euclidean")

TSFCA <- ac(p = philly20$totalpop, 
            n = evse$num, 
            D = dist.matrix, d0 = 2000, family = "2SFCA")
philly20 <- philly20 %>% 
  mutate(TSFCA = TSFCA)
```


## Local Environment

```{r load site environmental metrics, message=FALSE, warning=FALSE}

parking <- st_read(here("data", "raw", "osm_features", "parking.geojson"))
parking <- parking %>% 
  filter(!access %in% c("no", "permissive", "permit", "private")) %>% 
  st_transform('ESRI:102728')

police_fire <- st_read(here("data", "raw", "osm_features", "police_fire.geojson")) 
police_fire <- police_fire %>% 
  st_transform('ESRI:102728') %>% 
  st_centroid()

zoning <- st_read(here("data", "raw", "zoning", "zoning.shp"))
zoning <- zoning %>% 
  st_transform('ESRI:102728') %>% 
  mutate(zoning = case_when( ZONINGGROU == "Commercial/Commercial Mixed-Use" ~ "Commercial",
                             ZONINGGROU == "Industrial/Industrial Mixed-Use" ~ "Industrial",
                             ZONINGGROU == "Residential/Residential Mixed-Use" ~ "Residential",
                             ZONINGGROU == "Special Purpose" ~ "Special",
                             TRUE ~ "Undefined")) %>% 
  mutate(zone_score = case_when( zoning == "Commercial" ~ 0.4,
                             zoning == "Industrial" ~ 0.15,
                             zoning == "Residential" ~ 0.3,
                             TRUE ~ 0.15))
```

## Energy

```{r load property electricity data, message=FALSE, warning=FALSE}

electricity <- read.csv(here("data", "raw", "property-electricity-data.csv")) %>% 
  dplyr::select(Y = y_coord, X = x_coord, portfolio_bldg_id, postal_code, primary_property_type, sector, electricity_2021) %>%
  na.omit() %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102728') %>% 
  st_intersection(., fishnet) # clip to fishnet extent
```

# Aggregate Criteria

```{r aggregate all criteria into fishnet, message=FALSE, warning=FALSE}

net_centroid <- st_centroid(fishnet_nowater)
fishnet_nowater <- fishnet_nowater %>%
  left_join(net_centroid %>% 
              mutate(evse.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(evse), 2)*0.3048) %>% # distance to evse
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              st_intersection(philly20) %>% 
              st_drop_geometry(), by = "uniqueID") %>% # demographic info and spatial acc
  left_join(net_centroid %>% 
              st_intersection(reg_ev) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              mutate(parking.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(parking), 2)*0.3048) %>% # dist to parking
              st_drop_geometry(), by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              mutate(policefire.nn = nn_function(st_coordinates(net_centroid), 
                                           st_coordinates(police_fire), 1)*0.3048) %>% # police and fire
              st_drop_geometry(),by = "uniqueID") %>% 
  left_join(net_centroid %>% 
              st_buffer(dist = 200) %>% 
              st_intersection(zoning) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  mutate(TSFCA = ifelse(is.infinite(TSFCA), 0, TSFCA)) %>% 
  mutate(elec_cnt = ifelse(is.na(elec_cnt), 0, elec_cnt)) %>% 
  filter(is.na(GEOID) == FALSE) %>% 
  filter(is.na(zone_score) == FALSE) %>% 
  dplyr::select(-c(GEOID, totalpop, featCount, ZONINGGROU, zoning)) %>% 
  group_by(uniqueID) %>% # for duplicate grids, take the mean of the zone score
  mutate(zone_score = mean(zone_score)) %>%
  ungroup() %>%
  distinct(uniqueID, .keep_all = TRUE)
  
    
```


```{r aggregate power grid, message=FALSE, warning=FALSE}

# buffer the building and aggregate building centroid into the buffer
# if outside the buffer, get the nearest neighbor
elec_grid <- st_join(net_centroid, 
                electricity %>% st_buffer(dist = 1640)) %>% 
  group_by(uniqueID.x) %>% 
  summarize(elect_use = mean(electricity_2021))

unjoin <- elec_grid %>% 
  filter(is.na(elect_use)) %>% 
  mutate(neighbor_index = 1:n()) 
  
nn_indices <- st_nn(unjoin,
                    electricity, k = 1)
nearest_neighbors <- electricity[unlist(nn_indices), ] %>% mutate(neighbor_index = 1:n())

unjoin <- unjoin %>%
  st_drop_geometry() %>% 
  left_join(nearest_neighbors %>% select(electricity_2021, neighbor_index), 
            by = c("neighbor_index" = "neighbor_index"))

elec_grid <- elec_grid %>% 
  left_join(., unjoin %>% 
              dplyr::select(uniqueID.x, electricity_2021), by = c("uniqueID.x" = "uniqueID.x")) %>% 
  mutate(elect_use = case_when(
    is.na(elect_use) ~ electricity_2021,
    TRUE ~ elect_use
  )) %>% 
  dplyr::select(-electricity_2021) %>% 
  st_drop_geometry()

fishnet_nowater <- fishnet_nowater %>% 
  left_join(., elec_grid, by = c("uniqueID" = "uniqueID.x"))
```


```{r save all aggregated criteria}

st_write(fishnet_nowater, here("data", "derived", "all_criteria_raw.geojson"), driver = "GeoJSON")

```

# Multi-Criteria Decision Analysis

In it's simplest term, MCDA can be summarized into three steps.
1. Transform indicator data into standard **geographic unit**. This include: aggregate or dissolve for nested relationship, area-weighted re-aggregation for un-nested relationship, count points within polygons, etc. 
2. Transform indicator data into standard **measuring unit**. This include: rank or percentile, z-score, some other functions. 
3. Combine indicator data into a composite score with certain weights. 

## Self-Assigned Weight + WSM

The Weighted Sum Method is one of the simplest and most commonly used MCDA techniques. In WSM, each criterion is assigned a weight based on its importance, and each alternative is scored based on these criteria. The final score for each alternative is calculated by summing the products of the scores and their respective weights.

1. Scale all the criteria from 0 to 1. 
2. Invert the scale for certain criteria if necessary. 
3. Assign a weight to each criterion reflecting its relative importance. The sum of weights should equal 1.
4. For each alternative, multiply the performance score of each criterion by its weight and sum the results.
5. Rank the alternatives based on their total weighted scores.

```{r weighted sum}

scale_values <- function(x){(x-min(x))/(max(x)-min(x))}
WSM <- fishnet_nowater %>% 
  mutate(scl_density = scale_values(density),
         scl_acc = scale_values(TSFCA),
         scl_drivingpop = scale_values(driving_pop),
         scl_evse = scale_values(evse.nn),
         scl_elec = scale_values(elec_cnt),
         scl_parking = scale_values(parking.nn),
         scl_policefire = scale_values(policefire.nn),
         scl_elect_use = scale_values(elect_use)
         ) %>% # now inverse some scales
  mutate(scl_evse_re = 0 - scl_evse + 1,
         scl_acc_re = 0 - scl_acc + 1, 
         scl_parking_re = 0 - scl_parking + 1,
         scl_policefire_re = 0 - scl_policefire + 1) %>% 
  mutate(score = 0.12*scl_density + 0.18*scl_evse_re + 0.15*scl_elec + 0.1*scl_drivingpop + 0.1*scl_parking_re + 0.05*scl_policefire_re + 0.05*zone_score + 0.12*scl_acc_re +0.13*scl_elect_use) # self assign weight
```


```{r visualize weighted sum}

custom_palette <- c("#C4C4C4", "#B2BF95", "#80A676", "#88BFBF", "#7EA1BF")

quantiles <- classIntervals(WSM$score, n = 5, style = "quantile")
WSM$quantile <- cut(WSM$score, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=WSM, color="black", aes(fill=quantile)) + 
  scale_fill_manual(values = custom_palette) + 
  labs(title = "Weighted Sum Method Score", fill = "Bins") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
```

```{r save a version}

st_write(WSM, here("data", "derived", "WSM.geojson"), driver = "GeoJSON")

```


## Self Assigned Weight + TOPSIS

TOPSIS is a method that identifies solutions from a finite set of alternatives based on their geometric distance from an ideal solution. The ideal solution has the best performance values for all criteria, and the negative-ideal solution has the worst. This approach is employed widely for four main reasons: 1) the logic is rational and understandable, 2) the computation process is straightforward, 3) the concept permits the pursuit of the best alternatives for each criterion depicted in a simple mathematical form, 4) the importance weights are incorporated into the comparison procedure. 

1. Construct the decision matrix where each element represents the performance score of each alternatives.
2. Normalization
3. Determine the ideal and negative ideal solution. In other words, the best and worst possible value for each criterion. 
4. Calculate the separation measure as in euclidean distance calculation. 
5. Measure relative closeness: how close each alternative is to the ideal solution relative to its distance from the negative-ideal solution. 
6. Ranking 

```{r construct performance table}

performanceTable <- fishnet_nowater %>% 
  dplyr::select(-c(uniqueID)) %>% 
  st_drop_geometry() %>% 
  mutate(density = scale_values(density),
         TSFCA = scale_values(TSFCA),
         driving_pop = scale_values(driving_pop),
         evse.nn = scale_values(evse.nn),
         elec_cnt = scale_values(elec_cnt),
         parking.nn = scale_values(parking.nn),
         policefire.nn = scale_values(policefire.nn),
         elect_use = scale_values(elect_use)
         )
uniqueIDs <- fishnet_nowater$uniqueID
rownames(performanceTable) <- uniqueIDs
```


```{r topsis operations}

# reverse is min
weights <- c(0.18, 0.12, 0.1, 0.12, 0.15, 0.1, 0.05, 0.05, 0.13)
criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
# positiveIdealSolutions <- c(0.17, 0.60, 0.70, 0, 0.50, 0.10, 0.14, 0.3,0.007)
overall1 <- TOPSIS(performanceTable, weights, criteriaMinMax)
TOPSIS <- data.frame(uniqueID = names(overall1), values = overall1) %>%  
  mutate(uniqueID = as.integer(uniqueID))
  
TOPSIS_toplot <- fishnet_nowater %>% 
  left_join(TOPSIS, by = "uniqueID")

```


```{r visualize topsis result}

quantiles <- classIntervals(TOPSIS_toplot$values, n = 6, style = "quantile")
TOPSIS_toplot$quantile <- cut(TOPSIS_toplot$values, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=TOPSIS_toplot, color="black", aes(fill=quantile))+ 
  scale_fill_brewer(palette = "Blues") + 
  labs(title = "TOPSIS Score", fill = "Bins") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
```

However, the TOPSIS method presents certain drawbacks. One of the problems attributable to TOPSIS is that it can cause the phenomenon known as rank reversal. In this phenomenon the alternatives’ order of preference changes when an alternative is added to or removed from the decision problem. In some cases this may lead to what is called total rank reversal, where the order of preferences is totally inverted, that is to say, that the alternative considered the best, with the inclusion or removal of an alternative from the process, then becomes the worst. 


```{r save a version}

st_write(TOPSIS_toplot, here("data", "derived", "TOPSIS.geojson"), driver = "GeoJSON")

```


## Self Assigned Weights + PROMETHEE

PROMETHEE is a ranking method based on pairwise comparisons of alternatives according to each criterion, considering both preference and indifference thresholds. Preference threshold refers to the minimum difference between the performance of two alternatives on a given criterion such that the decision-maker has a strict preference for one alternative over the other. **If the performance difference is greater than or equal to p, the decision-maker clearly prefers one alternative over the other for that criterion.** The indifference threshold q represents the maximum difference between the performance of two alternatives on a given criterion such that the decision-maker is indifferent between them. **If the performance difference is less than or equal to q, the decision-maker considers the two alternatives to be effectively equal with respect to that criterion.** A preference function, which define how much better one site is over another for each criterion (a linear function where the lower the accessibility, the better), is then applied based on the two thresholds. 

1. Calculate the difference in performance scores for each criterion between all pairs of alternatives.
2. Set up an indifference threshold.
3. Set up a preference threshold. 
4. Decide a preference function
5. Apply the preference function using the indifference and preference threshold to convert the differences into preference value, which is typically betweeen 0 and 1. 
6. For each alternative, sum the preference values across all criteria to obtain the aggregated preference indices.
7. Calculate the positive and negative outranking flows for each alternative, which represent how much an alternative outranks others and is outranked by others, respectively.
8. Use the net outranking flow (positive flow minus negative flow) to rank the alternatives.

Now, this indifference and preference thresholds are based solely on quantitative analysis on the distribution of the data. We computed the IQR for all criteria and use that as the preference threshold. We divide the IQR by three and use that as the indifference threshold. 

```{r determine indifference and preference threshold}

IQR(performanceTable$evse.nn) / 3
IQR(performanceTable$density) / 3
IQR(performanceTable$driving_pop) /3 
IQR(performanceTable$TSFCA) / 3 
IQR(performanceTable$elec_cnt) / 3
IQR(performanceTable$parking.nn) / 3
IQR(performanceTable$policefire.nn) / 3
IQR(performanceTable$zone_score) / 3
IQR(performanceTable$elect_use) / 3

```


```{r promethee operations}

criteriaWeights <- c(0.18, 0.12, 0.1, 0.12, 0.15, 0.1, 0.05, 0.05, 0.13)
names(criteriaWeights)<-colnames(performanceTable)

criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
names(criteriaMinMax)<-colnames(performanceTable)

preferenceFunction<-c("Usual","U-shape","V-shape","Level","V-shape-Indiff","Gaussian", "Level","V-shape-Indiff", "Level")
# The gaussParameter vector in the code specifies the standard deviation (s) for the Gaussian preference 
# function for each criterion.
gaussParameter<-c(0.25,1,2,0,0,0,0,0,0)
names(gaussParameter)<-colnames(performanceTable)

#Preference threshold
preferenceThreshold<-c(0.31, 0.21, 0.12, 0, 0.17, 0.19, 0.23, 0.075, 0.005367344)
names(preferenceThreshold)<-colnames(performanceTable)

#Indifference threshold
indifferenceThreshold<-c(0.1, 0.07, 0.041, 0, 0.058, 0.065, 0.078, 0.025, 0.001789115)
names(indifferenceThreshold)<-colnames(performanceTable)

performanceTable <- as.matrix(performanceTable)
promethee <- PROMETHEEOutrankingFlows(performanceTable, preferenceFunction,preferenceThreshold,
indifferenceThreshold,gaussParameter,criteriaWeights,criteriaMinMax)
```


```{r clean up promethee table}

promethee_df <- data.frame(uniqueID = names(promethee$outrankingFlowsPos), 
           Pos = unname(promethee$outrankingFlowsPos),
           Neg = unname(promethee$outrankingFlowsNeg)) %>%  
  mutate(uniqueID = as.integer(uniqueID)) %>% 
  mutate(net = Pos-Neg) %>% 
  dplyr::select(uniqueID, net)

PROMETHEE_toplot <- fishnet_nowater %>% 
  left_join(promethee_df, by = "uniqueID")
  
```


```{r visualize promethee}
quantiles <- classIntervals(PROMETHEE_toplot$net, n = 6, style = "quantile")
PROMETHEE_toplot$quantile <- cut(PROMETHEE_toplot$net, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=PROMETHEE_toplot, color="black", aes(fill=quantile)) + 
  scale_fill_brewer(palette = "Blues") +
  labs(title = "PROMETHEE Score", fill = "Bins") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
```


```{r save a version}

st_write(PROMETHEE_toplot, here("data", "derived", "PROMETHEE.geojson"), driver = "GeoJSON")

```

## AHP + TOPSIS

To determine weights using the Analytic Hierarchy Process (AHP), start by defining the decision problem and structuring it into a hierarchy, including the goal at the top, followed by criteria and sub-criteria (if any), and the alternatives at the bottom. Construct pairwise comparison matrices for the criteria by comparing each pair and assigning relative importance values on a scale from 1 to 9. Normalize the matrices by dividing each element by the sum of its column, then calculate the priority vector (weights) by averaging the normalized values across each row. 



```{r set up pairwise matrix}

crit <- c("density","evse","ev", "drivingpop", "parking", "policefire", "zoning", "acc", "electricity")

criteriaWeightsPairwiseComparisons <- matrix(c(1.0, 3.0, 2.0, 1/2, 1/2, 1/4, 1/3, 1.5, 1.2,
                                               1/3, 1.0, 1/1.2, 1/4, 1/4, 1/8, 1/6, 1/2, 1/1.5, 
                                               1/2, 1.2, 1.0, 1/3, 1/3, 1/6, 1/6, 1/2, 1/1.5, 
                                               2, 4, 3, 1.0, 1/1.2, 1/4, 1/5, 1.5, 1.8,
                                               2, 4, 3, 1.2, 1.0, 1/4, 1/4, 2, 1.5, 
                                               4, 8, 6, 4, 4, 1, 1.2, 3, 2,
                                               3, 6, 6, 5, 4, 1/1.2, 1, 3, 2,
                                               1/1,5, 2, 2, 1/1.5, 1/2, 1/3, 1/3, 1, 1.5, 
                                               1/1.2, 1.5, 1.5, 1/1.8, 1/1.5, 1/2, 1/2, 1/1.5, 1
                                               ),
nrow=length(crit),
ncol=length(crit),
dimnames=list(crit,crit))


```


```{r compute weights based on the matrix}

column_sums <- colSums(criteriaWeightsPairwiseComparisons)

# Divide each element by the sum of its column
normalized_matrix <- sweep(criteriaWeightsPairwiseComparisons, 2, column_sums, FUN = "/")
weights <- rowMeans(normalized_matrix)

# Output the weights
print(weights)

```

```{r apply topsis operations}

# reverse is min
weights <- c(0.249, 0.114, 0.105, 0.098, 0.1909, 0.074, 0.033, 0.033, 0.102)
criteriaMinMax <- c("min", "max", "max", "min", "max", "min", "min", "max", "max")
overall2 <- TOPSIS(performanceTable, weights, criteriaMinMax)
AHP_TOPSIS <- data.frame(uniqueID = names(overall2), values = overall2) %>%  
  mutate(uniqueID = as.integer(uniqueID))
  
AHP_TOPSIS_toplot <- fishnet_nowater %>% 
  left_join(TOPSIS, by = "uniqueID") 


```


```{r visualize ahp results}

quantiles <- classIntervals(AHP_TOPSIS_toplot$values, n = 5, style = "quantile")
AHP_TOPSIS_toplot$quantile <- cut(AHP_TOPSIS_toplot$values, breaks = quantiles$brks, include.lowest = TRUE)

ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=AHP_TOPSIS_toplot, color="black", aes(fill=quantile)) + 
  scale_fill_manual(values = custom_palette) +
  labs(title = "AHP Score", fill = "Bins") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
  
```

```{r save a version}

st_write(AHP_TOPSIS_toplot, here("data", "derived", "AHP.geojson"), driver = "GeoJSON")

```


# Analyses

## Average Rank

```{r combine all ranks together}

TOPSIS <- TOPSIS %>%
  mutate(topsis_rank = min_rank(desc(values)))

WSM <- WSM %>% 
  dplyr::select(c(uniqueID, score)) %>% 
  mutate(wsm_rank = min_rank(desc(score)))

PROMETHEE <- promethee_df %>% 
  mutate(promethee_rank = min_rank(desc(net)))

AHP_TOPSIS <- AHP_TOPSIS %>% 
  mutate(ahptopsis_rank = min_rank(desc(values)))

rank_compare <- WSM %>% 
  dplyr::select(c(uniqueID, wsm_rank)) %>% 
  left_join(., TOPSIS %>% dplyr::select(uniqueID, topsis_rank), by = "uniqueID") %>% 
  left_join(., PROMETHEE %>% dplyr::select(uniqueID, promethee_rank), by = "uniqueID") %>% 
  left_join(., AHP_TOPSIS %>% dplyr::select(uniqueID, ahptopsis_rank), by = "uniqueID") %>% 
  mutate(avg_rank = as.integer((wsm_rank + topsis_rank + promethee_rank + ahptopsis_rank) / 4))

```


```{r save a version}

st_write(rank_compare, here("data", "derived", "average_rank.geojson"), driver = "GeoJSON")

```

We took the average rank among all methods because we would like to find places in Philadelphia that are consistently ranked more suitable than other. 

```{r visualize the average rank}

rank_toplot <- fishnet_nowater %>% 
  left_join(rank_compare, by = "uniqueID")
ggplot()+
  geom_sf(data=fishnet, color="black", fill="white") +
  geom_sf(data=rank_toplot, color="black", aes(fill=avg_rank)) +
  labs(title = "Average Rank", fill = "Rank") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
  
```

## Rank by Neighborhood

We would like to see if some neighborhoods are more suitable than others overall. 

```{r add neighborhood}

neighborhood <- st_read(here("data", "raw", "PhillyPlanning_Neighborhoods", "PhillyPlanning_Neighborhoods.shp")) %>% st_transform('ESRI:102728') 
  

final <- fishnet_nowater %>% 
  left_join(., rank_compare, by = "uniqueID") %>% 
  left_join(net_centroid %>% 
               st_intersection(neighborhood %>% dplyr::select(NAME)) %>% 
              st_drop_geometry(), by = "uniqueID") %>% 
  mutate(NAME = ifelse(is.na(NAME), "NOT APPLICABLE", NAME))
  
```


```{r neighborhood plot}

final %>% 
  st_drop_geometry() %>% 
  group_by(NAME) %>% 
  summarize(overall_rank = mean(avg_rank)) %>% 
  left_join(neighborhood, .,by = "NAME") %>% 
  ggplot()+
  geom_sf(color="black", aes(fill=overall_rank)) +
  scale_fill_gradient(low = "red", high = "white") + # Changed this line
  labs(title = "Neighborhood Rank", fill = "Rank") +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8)
        )
  
  
```

## Compare MCDA Methods

Comparing across TOPSIS, PROMETHEE, and weighted sum in ranking the alternatives by looking at the distribution of differences in the rank, we found that:
1. The differences in rank between weighted sum and promethee method is the smallest. 
2. TOPSIS is leading to rank reversal issues for some parts of Philadelphia. In other word, a few grids that were ranked of lower priority in PROMETHEE and WSM are ranked of much high priority in TOPSIS and vice versa. Closer examination of these grids reveal that they are located in the outskirt of Philadelphia, mainly industrial areas that use a lot of electricty power. TOPSIS assumes that criteria are independent of each other. When a new alternative is introduced or an existing one is removed, the distance to the ideal and negative-ideal solutions can change. The presence of extreme values (very high or very low) can significantly influence the ideal and negative-ideal solutions. MCDA is sensitive to the quality of our data. 
3. Assign weights directly has under-ranked several sites than using the AHP. 

```{r compare difference distribution, fig.height=6, fig.width=10}

# if negative, it means that wsm over-rank topsis or promethee
# if positive, it means that wsm under-rank topsis or promethee
compare<- final %>% 
  mutate(wsm_topsis = wsm_rank - topsis_rank,
         wsm_promethee = wsm_rank - promethee_rank,
         promethee_topsis = promethee_rank - topsis_rank) %>% 
  mutate(compare_weights = topsis_rank - ahptopsis_rank)

custom_palette1 <- c("#B2BF95", "#80A676", "#88BFBF", "#7EA1BF")

fig1 <- compare %>% 
  st_drop_geometry() %>% 
  dplyr::select(c(uniqueID, wsm_topsis, wsm_promethee, promethee_topsis, compare_weights, NAME)) %>%
  pivot_longer(cols = c("wsm_topsis", "wsm_promethee", "promethee_topsis", "compare_weights"),  
               names_to = "variable",        
               values_to = "value") %>% 
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 10, aes(fill = variable), alpha = 1) +
  labs(title = "Distribution of Differences in Rank Between Different Methods",
       x = "Difference",
       y = "Frequency") +
  facet_wrap(~ variable, labeller= labeller(variable = c(
    `promethee_topsis` = "PROMETHEE  vs. TOPSIS ",
    `wsm_topsis` = "WSM  vs. TOPSIS",
    `wsm_promethee` = "WSM  vs. PROMETHEE",
    `compare_weights` = "AHP and Directly Assign Weights"))) +
  scale_fill_manual(values = custom_palette1) + 
  theme(#axis.text.x=element_blank(),
        #axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "grey", fill=NA, size=0.8),
        legend.position = "none",  # Turn off the legend,
        strip.text = element_text(size = 12)
        )
  
ggsave("graph.png", fig1, width = 10, height = 6, dpi = 300)
```


```{r}

my_sort <- c("promethee_topsis", "wsm_topsis", "wsm_promethee", "compare_weights")



# Your data manipulation and ggplot code
plot <- compare %>% 
  st_drop_geometry() %>% 
  dplyr::select(c(uniqueID, wsm_topsis, wsm_promethee, promethee_topsis, compare_weights, NAME)) %>%
  pivot_longer(cols = c("wsm_topsis", "wsm_promethee", "promethee_topsis", "compare_weights"),  
               names_to = "variable",        
               values_to = "value") %>% 
  ggplot(aes(x = as.numeric(factor(variable, levels = my_sort)))) +
  geom_half_violin(aes(x = as.numeric(factor(variable, levels = my_sort)) + 0.1,
                       y = value, fill = factor(variable, levels = my_sort)),
                   side = 'r', scale = "width", cex = 0.8, lwd=0.3) +  
  geom_boxplot(aes(x = as.numeric(factor(variable, levels = my_sort)) + 0.1,
                   y = value, fill = factor(variable, levels = my_sort)),
               outlier.colour = "black", width = 0.1, cex = 0.8, lwd=0.3, outlier.size=0.6) +  
  geom_jitter(aes(x = as.numeric(factor(variable, levels = my_sort)) - 0.1,
                  y = value, color = factor(variable, levels = my_sort)),
              width = 0.1, size = 0.2, stroke = 0.8) +
  scale_fill_manual(values = custom_palette1) + 
  scale_color_manual(values = custom_palette1) +  
  labs(title = "Distribution of Differences in Rank Between Different Methods",
       x = "Method",
       y = "Difference") +
  theme(axis.text.x=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_blank(),
        legend.position = "none",  
        )

# Print the plot
print(plot)
ggsave("boxplot.png", plot, width = 10, height = 6, dpi = 300)


```




```{r fig.height=7, fig.width=10}

custom_palette2 <- c("#80A676", "white", "#7EA1BF")


fig2 <- compare %>% 
  dplyr::select(c(uniqueID, wsm_topsis, wsm_promethee, promethee_topsis, compare_weights, NAME)) %>%
  pivot_longer(cols = c("wsm_topsis", "wsm_promethee", "promethee_topsis", "compare_weights"),  
               names_to = "variable",       
               values_to = "value") %>% 
  filter(variable == "promethee_topsis" | variable == "wsm_topsis") %>% 
  ggplot() +
  geom_sf(aes(fill = value), color = "grey") +
  labs(title = "Ranking Difference between TOPSIS and Other Approaches") +
  facet_wrap(~ variable, labeller= labeller(variable = c(
    `promethee_topsis` = "PROMETHEE  vs. TOPSIS ",
    `wsm_topsis` = "WSM  vs. TOPSIS",
    `wsm_promethee` = "WSM  vs. PROMETHEE",
    `compare_weights` = "AHP and Directly Assign Weights"))) +
  scale_fill_gradientn(colors = custom_palette2, 
                       name = "Rank Difference", 
                       na.value = "grey50") + 
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks =element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9,face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank()    )
  
ggsave("plot.png", fig2, width = 10, height = 6, dpi = 300)

```

WSM: easy to understand and implement, but assumes that criteria are independent of each other and ranking is highly dependent on the weights

TOPSIS: easy to understand and implement and is more comprehensive than the simple WSM, but it also assumes that criteria are independent of each other, is sensitive to rank reveral issues, and requires additional input froms stakeholders to decide upon the positive and negative ideal scenario. 

PROMETHEE: the most robust statistical mode, but requires careful selection of preference functions and preference function, indifference threshold 

AHP: breaks complex decisions into manageable part, but pairwise comparison can be subjective and biased  and that maintaining consistency in comparison can be challenging. 

# Public Private Partnership

We decide to find places in Philadelphia that is consistently ranked as more suitable for new EVSE among all MCDA models. Through consulting the output of our  site visits, various stakeholder meetings, and budget considerations, we selected one of those sites in South Philadelphia and proposed a public-private partnership model between Philadelphia’s OIT and local grocery stores to install and maintain the EVSEs. We subsequently conducted financial analyses for the cost and revenue of breakdowns and designed a phased implementation of EVSE infrastructure given the current site conditions. 

```{r load acme locations}

acme <- read.csv(here("data", "raw", "acme.csv")) %>% 
  st_as_sf(., coords = c("Longitude", "Latitude"), crs = 4326) %>% 
  mutate(longitude= unlist(map(geometry, 1)),
         latitude = unlist(map(geometry, 2)))

```


```{r leaflet plots}

pal <- colorBin(palette = custom_palette, # change the color palette here
                domain = final$avg_rank, # change the parameter name after $
                bins = c(0, 200, 400, 600, 800, 1000, 1200)) # change the bins here 


labs <- paste0("<strong>", final$NAME,
               "</strong><br/>Avg Rank: ", final$avg_rank, 
               "</strong><br/>WSM Rank: ", final$wsm_rank, 
               "</strong><br/>TOPSIS Rank: ", final$topsis_rank, 
               "</strong><br/>PROMETHEE Rank: ", final$promethee_rank, 
               "</strong><br/>AHP-TOPSIS Rank: ", final$ahptopsis_rank
               ) 

labs_acme <- paste0("</strong><br/>ACME Address ", acme$Address)
acme_marker_color <- "black"
```

```{r}
leaflet <- final %>%
  st_transform("EPSG:4326") %>% 
  leaflet() %>%
  setView(lng = -75.1652, lat = 40.0126, zoom = 10.5) %>%
  addProviderTiles(providers$CartoDB.Positron) %>% 
  addPolygons(weight = 0.2,
              color = "black", 
              opacity = 0.5,
              fillColor = ~pal(avg_rank), # change the name of the parameter in the bracket
              popup = labs,
              fillOpacity = 2) %>%
  addCircleMarkers(
    data = acme,  # Replace with your actual point data
    lng = ~longitude,  # Replace with the appropriate variable in your point dataset for longitude
    lat = ~latitude,  # Replace with the appropriate variable in your point dataset for latitude
    weight = 0.5,
    radius = 4,
    fillColor = "black",  # Set the fill color for the markers
    fillOpacity = 1,
    popup = ~labs_acme  # Replace with the appropriate variable in your point dataset for popup text
  ) %>% 
  addLegend(position = "bottomright",
            pal = pal,
            values = ~avg_rank, # change the name of the parameter after ~
            title = "EVSE Suitability Rank") %>% 
  addControl(
    html = paste0('<div style="background: white; padding: 2px; border-radius: 2px;">
                    <i style="background:', acme_marker_color, '; width: 15px; height: 15px; display: inline-block; border-radius: 50%;"></i> ACME<br>
                    </div>'),
    position = "bottomright"
  )

leaflet
```

# Takeaways

GIS-based MCDA is a robust criteria based methodology that support multiple criteria and statistical models at once, which allows for more in-depth decision making in the planning field. 

Our study demonstrate a spatially informed starting point to identify potential areas for new EVSE that contribute to Philadelphia's sustainable transportation goals. 

However, our study also highlighted several challenges in using MCDA. This includes agreeing on the input criteria, the weighting schemes, and various other inputs required for MCDA models. Considering the number of goals for this particular decision, equity,sustainable transportation, affordability, etc.,  what should be included and how important they are become important questions to consider and would significantly alter the result. 

In addition, we would like to point out that there's no perfect decision making model. Methods that are more comprehensive and robust mathematically 1) requires more decision inputs, which is more time consuming, introduces more inconsistencies from stakeholders, and introduces more subjectivity 2) could be more computationally intensive, 3) less intuitive to non-experts. The best practice is to not rely on a single method in decision making. Beyond using MCDA models, it is essential to check the actual site condition before making final decisions. Note that there are things that spatial analysis cannot capture.

Moreover, our study have also highlighted several recurrent challenges in geospatial model for decision making. Specifically, the modifiable areal unit problem (MAUP) is a source of uncertainty, considering that out raw data all comes with different spatial unit and needs to be re-aggregated before proceed. In addition, ecological fallacy could be an issues as we are making inference about a small portion of the population using tract level statistics for the whole population.Moreover, errors and uncertainties may also arise when dealing with missing information, when we make assumptions/spatial interpolations about a neighboring grids' situation.









